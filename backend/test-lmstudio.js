const axios = require('axios');

const LM_STUDIO_URL = 'http://127.0.0.1:1234';

async function testLMStudio() {
  console.log('üß™ Testando conex√£o com LM Studio...\n');

  // Teste 1: Verificar modelos dispon√≠veis
  console.log('üìã Teste 1: Buscando modelos dispon√≠veis');
  try {
    // Tentar /v1/models primeiro
    let response;
    try {
      console.log('   Tentando GET /v1/models...');
      response = await axios.get(`${LM_STUDIO_URL}/v1/models`);
      console.log('   ‚úÖ Sucesso com /v1/models');
    } catch (error) {
      console.log('   ‚ö†Ô∏è  Falhou /v1/models, tentando /api/v1/models...');
      response = await axios.get(`${LM_STUDIO_URL}/api/v1/models`);
      console.log('   ‚úÖ Sucesso com /api/v1/models');
    }

    const models = response.data.data || response.data.models || [];
    console.log(`   üì¶ Modelos encontrados: ${models.length}`);
    models.forEach((model, index) => {
      console.log(`      ${index + 1}. ${model.id || model.name}`);
    });
    console.log('');
  } catch (error) {
    console.error('   ‚ùå Erro ao buscar modelos:', error.message);
    console.log('');
  }

  // Teste 2: Enviar mensagem de teste
  console.log('üí¨ Teste 2: Enviando mensagem "Oi" para o modelo');
  try {
    const payload = {
      model: 'local-model', // LM Studio geralmente aceita qualquer nome se houver apenas 1 modelo
      messages: [
        { role: 'system', content: 'Voc√™ √© um assistente √∫til.' },
        { role: 'user', content: 'Oi' }
      ],
      temperature: 0.7,
      max_tokens: 100,
    };

    // Tentar /v1/chat/completions primeiro
    let response;
    try {
      console.log('   Tentando POST /v1/chat/completions...');
      response = await axios.post(`${LM_STUDIO_URL}/v1/chat/completions`, payload);
      console.log('   ‚úÖ Sucesso com /v1/chat/completions');
    } catch (error) {
      console.log('   ‚ö†Ô∏è  Falhou /v1/chat/completions, tentando /api/v1/chat...');
      response = await axios.post(`${LM_STUDIO_URL}/api/v1/chat`, payload);
      console.log('   ‚úÖ Sucesso com /api/v1/chat');
    }

    const completion = response.data.choices[0].message.content;
    const tokensUsed = response.data.usage?.total_tokens || 0;

    console.log(`   ü§ñ Resposta: "${completion}"`);
    console.log(`   üìä Tokens usados: ${tokensUsed}`);
    console.log('');
  } catch (error) {
    console.error('   ‚ùå Erro ao enviar mensagem:', error.message);
    if (error.response) {
      console.error('   üìÑ Resposta do servidor:', error.response.data);
    }
    console.log('');
  }

  // Teste 3: Verificar todas as rotas dispon√≠veis
  console.log('üîç Teste 3: Verificando rotas dispon√≠veis');
  const routes = [
    '/v1/models',
    '/api/v1/models',
    '/v1/chat/completions',
    '/api/v1/chat',
    '/api/v1/completions',
    '/v1/completions',
  ];

  for (const route of routes) {
    try {
      if (route.includes('chat') || route.includes('completions')) {
        // POST routes
        await axios.post(`${LM_STUDIO_URL}${route}`, {
          model: 'test',
          messages: [{ role: 'user', content: 'test' }],
          max_tokens: 1,
        }, { timeout: 2000 });
        console.log(`   ‚úÖ ${route} (POST)`);
      } else {
        // GET routes
        await axios.get(`${LM_STUDIO_URL}${route}`, { timeout: 2000 });
        console.log(`   ‚úÖ ${route} (GET)`);
      }
    } catch (error) {
      if (error.code === 'ECONNABORTED') {
        console.log(`   ‚è±Ô∏è  ${route} (timeout)`);
      } else if (error.response && error.response.status !== 404) {
        console.log(`   ‚ö†Ô∏è  ${route} (${error.response.status})`);
      } else {
        console.log(`   ‚ùå ${route} (n√£o dispon√≠vel)`);
      }
    }
  }

  console.log('\n‚ú® Testes conclu√≠dos!');
}

testLMStudio().catch(console.error);
